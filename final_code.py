# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptfDV5uCaQwlSiqe69cIMZbIK_BfzGJ1
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/ms4610project"

import warnings 
warnings.filterwarnings('ignore')

import pandas as pd
import seaborn as sns
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

df = pd.read_csv("Training Data_2021.csv")
test_df = pd.read_csv("Test Data_2021.csv")
#df['default_ind'].value_counts() 0-59145 and 1-23855
#df['mvar47'].value_counts()    C-52043 and L-30957
#df['mvar47'].nunique()

df.head(3)

df.info()

"""## Get all missing values as np.NaN

"""

df = df.replace('missing', np.NaN)
df = df.replace('na', np.NaN)
test_df = test_df.replace('missing', np.NaN).replace('na', np.NaN)

def get_int(x):
  if x is np.NaN:
    return x 
  else :
    return int(x)

def get_float(x):
  if x is np.NaN:
    return x 
  else :
    return float(x)

df_new  = df.copy()
df_new.columns

test_df_new = test_df.copy()

# get the float values for columns that can be float 
float_columns= ['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6','mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13','mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20',
'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27','mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34','mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41',
'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46'
]
for column in float_columns:
  df_new[column]= df_new[column].apply(get_float)
  test_df_new[column] = test_df_new[column].apply(get_float)

df.head(3)

colums =[ 'mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6','mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13','mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20',
'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27','mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34','mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41',
'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46','mvar47']

feature_unique_values = []
for feature in colums:
  temp = df_new[df_new[feature] != np.NaN].copy()
  num_uniqe_values = temp[feature].unique().__len__()
  feature_unique_values.append({
      "feature":feature, "num_unique_values" : num_uniqe_values
  })

feature_unique_values_df = pd.DataFrame(feature_unique_values)
feature_unique_values_df.head(3)

import numpy as np

def show_values_on_bars(axs):
    def _show_on_single_plot(ax):        
        for p in ax.patches:
            _x = p.get_x() + p.get_width() / 2
            _y = p.get_y() + p.get_height()
            value = '{:.1f}'.format(p.get_height())
            ax.text(_x, _y, value, ha="center", rotation=60) 

    if isinstance(axs, np.ndarray):
        for idx, ax in np.ndenumerate(axs):
            _show_on_single_plot(ax)
    else:
        _show_on_single_plot(axs)



sns.set(font_scale=1.4)
plt.figure(figsize=(32,5))
ax =sns.barplot(data = feature_unique_values_df.sort_values(by='num_unique_values'),
                x ='feature', y ='num_unique_values'
            )
ax.set_xticklabels(feature_unique_values_df.sort_values(by='num_unique_values')['feature'],rotation=30)
plt.xlabel("feature column ")
plt.ylabel("number of unique values  in the entire column")
show_values_on_bars(ax)

from tqdm.notebook import  tqdm

feature_num_null_values = []
for feature in tqdm(colums):
  num_null_values = df[feature].isna().sum()
  feature_num_null_values.append({
      "feature":feature, "num_null_values" : num_null_values
  })
feature_num_null_values_df = pd.DataFrame(feature_num_null_values)
feature_num_null_values_df['num_null_values_percent'] = feature_num_null_values_df.num_null_values/830
feature_num_null_values_df.head(3)

sns.set(font_scale=1.4)
plt.figure(figsize=(32,5))
ax =sns.barplot(data = feature_num_null_values_df.sort_values(by='num_null_values'),
                x ='feature', y ='num_null_values_percent'
            )
ax.set_xticklabels(feature_num_null_values_df.sort_values(by='num_null_values')['feature'],rotation=30)
plt.xlabel("feature column ")
plt.ylabel("% of null values  in the entire column")
show_values_on_bars(ax)

feature_num_null_values_df[feature_num_null_values_df['feature']=='mvar31']

feature_num_null_values_df.sort_values(by='num_null_values')[37:]['feature'].to_list()

!pip install missingno

import missingno as msno
msno.matrix(df_new)

import missingno as msno
msno.matrix(test_df_new)

"""# Summary Of each columns before imputation"""

temp =  df.copy().replace("missing", np.NaN).replace("na", np.NaN)
for x in temp.columns:
  print(f"=================================\n {x}")
  print(temp[x].describe())

"""# Getting categorical columns"""

remove_features = ['mvar22','mvar26',
 'mvar15',
 'mvar35',
 'mvar30',
 'mvar23',
 'mvar45',
 'mvar11',
 'mvar41',
 'mvar31',
 'mvar40']

feature_unique_values_df_filtered =feature_unique_values_df[feature_unique_values_df['feature'].apply(lambda x : x not in remove_features)]

dist_hist_threshold =  100
categorical_features = feature_unique_values_df_filtered[feature_unique_values_df_filtered['num_unique_values']<dist_hist_threshold].sort_values(by='num_unique_values')['feature'].to_list()
quantitative_features =  feature_unique_values_df_filtered[feature_unique_values_df_filtered['num_unique_values'] > dist_hist_threshold].sort_values(by='num_unique_values')['feature'].to_list()
print(f"number of categorical feature {len(categorical_features)}  number of quatitative features {len(quantitative_features)}")

rows = 4
cols = 5
sns.set(font_scale=1.2)
fig,ax =  plt.subplots(rows,cols,figsize=(30, 20))
for i in range(rows):
  for j in range(cols):
    if i*cols+j<len(categorical_features):
      feature = categorical_features[i*cols+j]
      temp = df_new[df_new[feature] != np.NaN].copy()
      x = temp[feature].value_counts().index
      y = temp[feature].value_counts().values
      sns.barplot(ax= ax[i,j] ,x = x, y= y)
      ax[i,j].set_title(f"{feature}") 
plt.show()

rows = 5
cols = 5
sns.set(font_scale=1.2)
fig,ax =  plt.subplots(rows,cols,figsize=(30, 20))
for i in range(rows):
  for j in range(cols):
    if i*cols+j<len(quantitative_features):
      feature = quantitative_features[i*cols+j]
      temp = df_new[df_new[feature]!= np.NaN].copy()
      sns.distplot(ax= ax[i,j] ,x = temp[feature])
      ax[i,j].set_title(f"{feature}") 
plt.show()

"""# Transformation after imputing with Mean """

test_df_imputed = test_df_new.copy()

# imputation with mean and median 
df_imputed = df_new.copy()
for feature in quantitative_features:
  df_imputed[feature].fillna(value =df_imputed[feature].mean(), inplace=True)
  test_df_imputed[feature].fillna(value =df_imputed[feature].mean(), inplace=True)

print(df_imputed['mvar46'].unique())
df_imputed['mvar46'].value_counts()

df_imputed['mvar46'] = df_imputed['mvar46'].fillna(df_imputed.mvar46.mode()[0])

for feature in categorical_features:
  df_imputed[feature] = df_imputed[feature].fillna(value =df_imputed[feature].mode()[0])
  test_df_imputed[feature] = test_df_imputed[feature].fillna(value =df_imputed[feature].mode()[0])





"""## Plots After Imputation with mean and mode streategy """

feature_num_null_values_df.head()

quantitative_features_sorted= feature_num_null_values_df[feature_num_null_values_df.sort_values(by='num_null_values').feature.apply(lambda x : x in quantitative_features)].sort_values(by='num_null_values')['feature'].to_list()

# feature_num_null_values_df[feature_num_null_values_df.sort_values(by='num_null_values').feature.apply(lambda x : x in quantitative_features)].sort_values(by='num_null_values')

from sklearn.preprocessing import  StandardScaler

df_new[df_new['mvar14']<df_new['mvar14'].quantile(0.90)]['mvar14'].plot(kind='hist')

sns.set(font_scale=1.2)
for feature in quantitative_features_sorted:
  fig,ax =  plt.subplots(1,2,figsize=(30, 3))
  temp = df_new[df_new[feature] != np.NaN].copy()
  sns.distplot(ax= ax[0] ,x = temp[feature])
  ax[0].set_title(f"{feature} before imputation") 
  sns.distplot(ax= ax[1] ,x = df_imputed[feature])
  ax[1].set_title(f"{feature} after imputation") 
  plt.show()

sns.set(font_scale=1.2)
for feature in categorical_features:
  fig,ax =  plt.subplots(1,2,figsize=(30, 3))
  temp = df_new[df_new[feature] != np.NaN].copy()
  value_counts = temp[feature].value_counts()
  sns.barplot(ax= ax[0] ,x =value_counts.index, y= value_counts.values)
  ax[0].set_title(f"{feature} before imputation") 
  value_counts = df_imputed[feature].value_counts()
  sns.barplot(ax= ax[1],x =value_counts.index, y= value_counts.values)
  ax[1].set_title(f"{feature} before imputation ") 
  plt.show()

"""# Target Value Distribution """

import  plotly.express as px 

# sns.histplot(df_new['default_ind'].value_counts(),bins=2)

df_new['default_ind'].value_counts()

fig = sns.barplot(x=['0', '1'], y=[59145/830,23855/830])
plt.xlabel("class")
plt.ylabel("% of sample")
plt.title("class distribution")
plt.show()

df_imputed['mvar47_encoded'] = df_imputed['mvar47'].apply(lambda x :1 if x=='C' else 0)

input_features = [ 'mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6',
       'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13',
       'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20',
       'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27',
       'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34',
       'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41',
       'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46' , 'mvar47_encoded'
       ]

target_features = ['default_ind']

from sklearn.model_selection import train_test_split
x , y = df_imputed[input_features].drop(columns=remove_features), df_imputed[target_features]
x_train, x_val, y_train, y_val         = train_test_split(x,y,test_size=0.25)

fig , ax = plt.subplots(1,2,figsize = (14,5))
sns.histplot(y_train, ax=ax[0],bins=2)
ax[0].set_title("train target distribution")
sns.histplot(y_val, ax=ax[1],bins=2)
ax[1].set_title("test target distribution")
plt.show()

"""# Test to Classification models"""

from sklearn.metrics import accuracy_score, log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

classifiers = {
    "LogisticRegression":LogisticRegression(),
    "KNeighborsClassifier":KNeighborsClassifier(3),
   # "SVC":SVC(kernel="rbf", C=0.025, probability=True),
    "DecisionTreeClassifier":DecisionTreeClassifier(),
    "RandomForestClassifier":RandomForestClassifier(),
    "AdaBoostClassifier":AdaBoostClassifier(),
    "GradientBoostingClassifier":GradientBoostingClassifier(),
    "GaussianNB":GaussianNB()
    }

def evaluate_model(model):
  model.fit(x_train, y_train)
  y_pred_train = model.predict(x_train)
  y_pred_val = model.predict(x_val)
  print(f"training accuracy {accuracy_score(y_train,y_pred_train)} test accuracy {accuracy_score(y_val, y_pred_val)}")

  y_pred_train = model.predict_proba(x_train)
  y_pred_val = model.predict_proba(x_val)
  print(f"training log loss {log_loss(y_train,y_pred_train)} test log loss {log_loss(y_val, y_pred_val)}")
  return model

for key in classifiers:
  print(f"=====================================\n {key}")
  evaluate_model(classifiers[key])

for max_depth in [20]:
  print(f"=====================================\n {n_estimators}")
  random_forest_classifer = RandomForestClassifier(n_estimators=100,criterion='gini' , max_depth=max_depth )
  evaluate_model(random_forest_classifer)

lda = LinearDiscriminantAnalysis()
evaluate_model(lda)

from xgboost import XGBClassifier
xgb_classifier = XGBClassifier()

evaluate_model(xgb_classifier)

x_test =test_df_imputed[input_features]
y_test  = random_forest_classifer.predict(x_test)

test_df_imputed['default_ind'] = y_test

test_df[['application_key','default_ind']].to_csv("group29_1.csv", index=False)

"""# Variable Relevances """

!pip install shap

import shap

explainer =

"""# Imputation with Knn"""

from sklearn.impute import KNNImputer

df_imputed_knn = df_new.copy()

imputer = KNNImputer()

df_imputed_knn.columns

features = ['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6',
       'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13',
       'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20',
       'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27',
       'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34',
       'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41',
       'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46']

imputer.fit_transform(df_imputed_knn[features])

"""# Explaining variables """